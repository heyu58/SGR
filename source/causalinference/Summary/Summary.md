A First Course in Causal Inference's Summary
--------------------

因果关系是我们人类探索世界时最值得深挖的东西。在自然科学的各个领域，无数伟大的规律，都是科学家通过精心设计的实验，一步步揭开事物之间“谁导致谁”的秘密。比如，物理学里的经典力学告诉我们力如何导致运动；化学里的元素周期律揭示了元素性质背后的因果顺序；生物学里的进化论，则通过自然选择解释了物种为什么会一代代变化。我们总是从证据入手，去追寻现实世界中那些隐藏的因果链条。

而在统计学里，我们的一切推理都从“数据”开始。数据就像是从真实世界总体中抽取的一小把样本，里面藏着我们最想知道的信息。当我们面对数据里可能透露出的因果关系时，通常会有两种主要的思路：一种叫**因果发现（Causal Discovery）**，另一种叫**因果推断（Causal Inference）**。

这两者到底有什么不一样呢？

**因果发现**有点像数据挖掘，或者说像考古探险。一切都还是未知的，我们手里只有一堆数据，就像考古学家面对一堆碎陶片和古文物一样，试图从中拼凑出事情的原貌：哪些变量互相影响？谁是因，谁是果？有没有更深层次的规律藏在里面？我们完全是从零开始探索，目标是找出可能的因果结构。

**因果推断**则完全不同。它通常出现在我们已经对因果方向有了初步判断的情况下。比如，我们相信某个“处理”（Treatment，比如吃某种药、上某种课、实施某项政策）会影响某个“结果”（Outcome，比如身体是否好转、成绩是否提高、收入是否增加），而且结果往往发生在处理之后。这时，我们最关心的问题就变成了：这个处理到底对结果产生了多大的影响？影响是正向的还是负向的？中间有没有其他变量在起作用？我们该怎么科学地量化这种“处理 → 结果”的因果效应？这正是因果推断的核心任务。

因果推断其实是一门相当年轻的学科。早在1923年，统计学家Neyman就在他的硕士论文中首次提出了“潜在结果”（potential outcomes）的想法，用来描述完全随机化实验下的因果效应。后来，到1980年，统计学家Rubin系统地整理和发展了这些思想，形成了今天广为人知的**鲁宾因果框架（Rubin Causal Model）**。进入21世纪后，因果推断开始在经济学、社会学、医学、公共政策等社会科学领域大放异彩，帮助人们更严谨地回答“如果做了A，会不会真的带来B”的问题。可惜的是，在国内，对因果推断的重要性和方法的认识还远远不够，很多研究依然停留在“相关关系”的层面。

希望通过这样的介绍，小伙伴们能对因果关系有个更直观的感受：它不仅仅是哲学思辨，更是可以用数据和科学方法一步步逼近的真实！

------------------------

完全随机化实验
------------------------
从实验设计的角度来看，完全随机化实验（Completely Randomized Experiment）是探究因果效应最可靠的方法。它建立在有限总体的框架下，唯一的随机性来源于实验者对处理变量$Z$的随机分配，而每个单位的潜在结果$Y(1)$和$Y(0)$本身是固定的、没有随机性的。

$$Z\perp\!\!\!\perp\{Y(1),Y(0)\}$$

这个独立性条件对我们正确进行因果推断帮助极大，它让处理分配不会受到潜在结果的影响，从而避免了混淆。

完全随机化实验给了我们两种经典的分析思路：

* Fisher的思路：他关心如何严格检验“处理完全没有效果”的原假设$H_{0F}:Y_i(1)=Y_i(0)\ \forall i$（即每个单位处理前后结果都一样）。他给出了利用任意检验统计量得到$p_{FRT}=\dfrac{1}{M}\sum_{m=1}^MI\{T(z^m,Y)\geq T(Z,Y)\}$的方法。一个典型的统计量是两个有限总体的均值差异$\hat{\tau}=\hat{\overline{Y}}(1)-\hat{\overline{Y}}(0)$。这实际上把问题转化成了经典的两样本均值差异检验。  
  在实际操作中，我们常用近似的t检验或非参数检验
  ```
  t.test(Y[Z==1],Y[Z==0],var.equal = FALSE)
  #或者wilcox秩和检验
  ```
* Neyman的思路：他更关注平均因果效应的点估计及其不确定性。他给出了平均因果作用的概念，以及无偏的点估计。$E[\hat{\tau}]=\tau=\dfrac{1}{n}\sum_{i=1}^n\tau_i$，虽然方差的精确估计中有一部分无法计算（因为我们永远观测不到同一个单位的两个潜在结果），但Neyman给出了一个保守（偏大）的方差估计，这在最坏情况下也能保证我们的置信区间覆盖真实值，仍然非常有意义：$E[\hat{V}]-var(\hat{\tau})=\dfrac{S(\tau)}{n}\geq 0$。我们通常可以利用一个简单的线性回归得到估计量点估计，$\hat{\beta}=\hat{\tau}(simple\ matrix\ calculate)$，而方差最好用稳健的标准误（EHW，异方差稳健）来估计。
  ```
  方法1：tauhat = mean(y[z==1])-mean(y[z==0])
        varhat = var(y[z==1])/n1+var(y[z==0])/n0
  方法2: olsfit = lm(y~z)
        tauhat = olsfit$coef
        varhat = hccm(olsfit,type = "hc2")
  ```
如果实验中只有$Z$（处理）和$Y$（结构），在完全随机化假设下，上面这些方法已经足够好。但如果我们还有合适的协变量$X$，这些额外信息能帮助我们更精确地估计因果效应。数学上，分层随机化实验（Stratified Randomized Experiment）的估计方差总是小于或等于完全随机化：$Var_{SRE}(\hat{\tau})\leq Var_{CRE}(\hat{\tau})$。所以，只要协变量选择得当，分层随机化总是更优。

然而，当协变量$X$是多维的、连续的，或者维度很高时，设计有效的分层规则会变得非常困难，甚至几乎不可能。这时**回归调整（Regression Adjustment）**给了我们利用协变量的机会。其中一个广受推荐的方法是Lin(2013)提出的估计量$Lin(2013)'s\  estimator$。通过估计潜在结果中无法用协变量解释部分的差异$\hat{\tau}(\beta_1,\beta_0)=\{\hat{\overline{Y}}(1)-\beta_1\hat{\overline{X}}(1)\}-\{\hat{\overline{Y}}(0)-\beta_0\hat{\overline{X}}(0)\}$，这个方法的核心是先把所有协变量标准化（中心化到均值为0），无论它们是离散还是连续的，这样才能满足推导中的假设。然后，只需拟合一个包含交互项的OLS回归，就能直接得到调整后的因果效应估计：
```
obtain the hattau(b1,b0) from a single OLS fit
hattau(b1,b0) = coefficient of Z in the OLS of Y ~ Z + X + Z*X
where X = scale(X)
```

通过这些方法，我们即使在复杂的现实数据中，也能更稳健、更精确地回答“这个处理真正产生了多大的因果效果”这个问题。

----------------------------------------

unconfounded观察性研究
---------------------------------------
在观察性研究中（Observational Study），我们无法像实验那样通过随机分配来实现处理$Z$与潜在结果$\{Y(1), Y(0)\}$的完全独立——那个理想的$Z\perp\!\!\!\perp\{Y(1),Y(0)\}$在我们手里根本实现不了。我们只能从已有的数据结果来尝试因果推断。

为了让因果推断还有一线希望，我们不得不绝望地提出一个相当强的假设——**强忽略性假设**（Strong Ignorability）：

$$\{Y(1),Y(0)\}\perp\!\!\!\perp Z|X$$

意思是：在控制了足够的协变量$X$之后，处理分配$Z$与潜在结果不再相关。换句话说，只要我们把所有可能同时影响处理$Z$和结果$Y$的混杂因素（confounders）都纳入$X$中，就没有隐藏的混杂干扰了。这个假设虽然听起来美好，但在现实中往往难以完全验证，我们只能尽力收集尽可能多的协变量来让它更可信。

在强忽略性假设成立的前提下，观察性数据中常用的因果效应估计方法主要有以下三种：

* 结果回归$Outcome\ Regression：$对结果变量使用回归模型是很直觉的，回归模型是统计学家最熟悉的。如果认为线性模型是合适的，就有$E[Y|Z,X]=\beta_0+\beta_zZ+\beta_xX$，这种情况下想要得到$\tau(X)=E[Y|Z=1,X]-E[Y|Z=0,X]$ 无异是简单的。觉得线性回归不好，试试别的也可以。这样松散的要求导致结果对模型的选择十分敏感。
* $IPW:$ 这条路子完全从“处理分配”入手。先估计倾向得分（propensity score）$e(X) = P(Z=1 \mid X)$，然后在另一个关键假设**重叠（Positivity 或 Overlap）**：$e(X) \in (0,1)$成立的前提下，平均处理效应可以表示为 $\tau=E\{Y(1)-Y(0)\}=E\left\{\dfrac{ZY}{e(X)}-\dfrac{(1-Z)Y}{1-e(X)}\right\}$，实际中，通常用逻辑回归`pr(Z=1|X) = logistic(Z ~ X)`来拟合倾向得分，再代入计算IPW估计量。这个方法的好处是不需要对结果$Y$建模，但倾向得分在边界附近（接近0或1）时权重会变得极大，导致估计量方差很大、极不稳定。实操中常常需要对权重进行截断（truncation）来改善表现。
* $Doubly\ Robust\ Esitimator：$这是一种聪明地把前面两种方法结合起来的策略。它同时利用结果回归模型$\mu(X, \beta)$和倾向得分模型$e(X, \alpha)$来构建估计量。最妙的地方在于：只要这两个模型中**至少有一个是正确指定的**，双重稳健估计量就能保持对平均处理效应的无偏性。这大大提高了我们在模型不确定时的保险系数——它“双重保险”，大大降低了完全错估的风险。

通过这些方法，我们即使没有随机化实验，也能在观察性数据中尽可能可靠地估计因果效应。当然，所有方法都依赖强忽略性等假设是否近似成立，这也是为什么在观察性研究中，我们永远要比随机实验更谨慎地解释结果。

------------------------------

confounded观察性研究
---------------------------------
如果存在我们无法观测到的混杂因素（Unmeasured Confounding），情况就变得棘手了。这时，之前所有依赖**强忽略性假设**的方法都会失效——因为我们根本没有把所有可能同时影响处理$Z$和结果$Y$的变量都控制住，隐藏的混杂会悄悄扭曲我们的因果估计。

不幸的是，我们永远无法用数据直接验证强忽略性假设是否真正成立。它更像是一个“信念跃迁”。一旦这个假设被打破，我们就需要其他工具来评估结果的可靠性。这时，**敏感性分析（Sensitivity Analysis）** 就成了守护因果结论的重要防线，它帮助我们回答：“到底需要多强的未观测混杂，才能把我们观察到的因果效应完全推翻？”

常见敏感性分析方法包括：

* **E-Value**  
  假设存在一个未观测的混杂因素$U$，并且在控制$U$后处理与结果独立（即$Z \perp\!\!\!\perp Y \mid (X, U)$）。E-Value给出这样一个下界：要完全解释掉我们观察到的关联，至少需要一个未观测混杂$U$，它与处理$Z$和结果$Y$的关联强度（用风险比RR表示）最大值至少达到  

  $$ \max(RR_{ZU}, RR_{UY}) \geq RR_{ZY}^{obs} + \sqrt{RR_{ZY}^{obs}(RR_{ZY}^{obs}-1)} $$ 

  右边这个值就是E-Value。E-Value越大，说明我们的因果结论越稳健——需要一个非常强的未观测混杂才能颠覆它。相比Fisher在随机实验中给出的p值，E-Value为观察性研究提供了一种更直观的“因果强度”衡量标准。

* **基于参数方法的敏感性分析**  
  在结果回归（Outcome Regression）、IPW、双重稳健估计量等方法中，我们也可以引入额外的参数来模拟未观测混杂的影响程度，系统地考察结论在不同混杂强度下的变化范围。

一般来说，多纳入一些可观测的协变量$X$，能让强忽略性假设更可信。但另一个关键假设——**重叠假设（Overlap）**——在现实中却经常出问题。某些个体的倾向得分$e(X)$可能非常接近0或1，甚至正好等于0或1（比如某些背景的人几乎不可能接受处理）。这会导致权重爆炸或样本丢失。

当重叠假设严重违反时，一种广受认可的替代方法是**断点回归（Regression Discontinuity, RD）**：

* **RD设计**  
  当处理分配$Z$在某个连续变量$X$的阈值$x_0$处突然跳跃（例如分数超过60分才能获得奖学金），我们就可以在阈值附近比较两侧的平均结果，从而估计局部平均因果效应：  

  $$\tau(x_0) = \lim_{\epsilon \to 0^+} E[Y \mid Z=1, X=x_0 + \epsilon] - \lim_{\epsilon \to 0^+} E[Y \mid Z=0, X=x_0 - \epsilon]$$  

  在阈值附近，个体的特征分布应该几乎相同，唯一的差异就是是否越过阈值获得处理。这相当于一个“准实验”，可以看作是对连续性假设的敏感性检验。

最后，如果我们想从**实验设计**的角度主动对抗未知混杂，一个强大工具是**工具变量**（Instrumental Variable, IV）方法：

* **激励设计（Encouragement Design）**  
  我们引入一个随机激励$Z$（比如随机发放优惠券鼓励接受治疗$D$），但最终是否接受治疗$D$仍由个体决定。通过联合潜在结果$\{D_i(1), D_i(0)\}$，我们可以把人群分为四类（始终接受者always-takers、顺从者compliers、反抗者defiers、从不接受者never-takers）。在合理假设下（单调性、无反抗者等），随机激励$Z$允许我们无偏估计**顺从者**的局部平均处理效应（LATE）：  
  $$\tau_c = \dfrac{\tau_Y}{\tau_D} = \dfrac{E(Y \mid Z=1) - E(Y \mid Z=0)}{E(D \mid Z=1) - E(D \mid Z=0)}$$

* **线性工具变量模型**  
  在计量经济学框架中，我们用因果图明确潜在混杂的影响路径，然后通过**两阶段最小二乘法**（Two-Stage Least Squares, TSLS）来估计工具变量效应，从而绕过未观测混杂，直接识别因果效应。

-----------------------------------------

主分层方法与中介分析
--------------------------------------
在很多实际应用中，会出现一种特殊的变量：它在接受处理**之后**才发生变化（比如生存分析中的“是否存活”、教育研究中的“是否完成课程”、医疗研究中的“是否出现副作用”等）。这类变量通常被称为**后处理变量**（Post-treatment Variable），记作$M$。它会与最终结果$Y$强烈相关，研究它在因果机制中的作用非常重要——因为它可能正是处理$Z$影响$Y$的关键中间环节，或者本身就是我们关心的异质性来源。

对于后处理变量，一种经典的分析思路是**主分层方法（Principal Stratification）**。这个方法的思想非常直观：既然$M$会受到处理$Z$的影响，我们就按$M$在不同处理下的潜在取值把人群分成几个“隐藏的层”（strata），然后在每个层内比较处理的平均因果效应。

以二元$M$为例，最受关注的一类人群是**顺从者**（compliers）：那些在处理下$M(1)=1$、但不处理时$M(0)=0$的人。对于这部分人群，我们可以定义主层因果效应：  
$$\tau(1,0)=E\{Y(1)-Y(0)\mid M(1)=1,M(0)=0\}$$  
类似地，我们还能定义其他三类：  
- $\tau(1,1)$：始终会出现$M=1$的人群（always-takers）  
- $\tau(0,1)$：反抗者（defiers，实际中常假设不存在）  
- $\tau(0,0)$：从不出现在$M=1$的人群（never-takers）  

这种分层方式和前面激励设计（Encouragement Design）中对人群的分类非常相似，只不过这里分层依据的是**受处理影响的中间变量$M$**，而不是最终是否接受处理$D$。

另一种常见的研究场景是，我们明确想探讨处理$Z$到结果$Y$的因果路径上是否存在**中介因素**（Mediator）。这时就需要进行**中介分析（Mediation Analysis）**。

中介分析的核心工具是嵌套潜在结果（nested potential outcomes）：$Y(z, M_z)$，表示在人为设定处理为$z$、同时中介变量被设定为在处理$z$下本应取的值时的结果。

在**组合假设**（Composition Assumption，或称一致性假设的扩展）$Y(z, M_z) = Y(z),\ z=0,1$成立的前提下，我们可以干净地将总因果效应$\tau$分解为两条路径：

- **自然直接效应（Natural Direct Effect, NDE）**：处理$Z$通过不经中介的路径直接影响$Y$的部分  
  
  $$NDE = E\{Y(1, M_0) - Y(0, M_0)\}$$  

  （即把中介固定在不处理时的自然水平，比较处理与不处理的差异）

- **自然间接效应（Natural Indirect Effect, NIE）**：处理$Z$通过改变中介$M$进而影响$Y$的部分  
  
  $$NIE = E\{Y(1, M_1) - Y(1, M_0)\}$$  
  
  （即固定处理为1，比较中介在自然取$M_1$与固定为$M_0$时的差异）

在组合假设下，总平均处理效应正好等于直接效应加间接效应：
  
$$\tau = NDE + NIE$$

中介分析帮助我们回答“处理到底是通过什么机制影响结果的？多少效果是直接的，多少是通过中介传导的？”这类机制性问题，而主分层方法则更侧重于处理后处理变量导致的异质性，以及在特定潜在人群中的纯净效应。

两者都是处理后处理变量的强大工具，选择哪种取决于研究目标：是想**避开**后处理变量的混杂（主分层），还是想**深入挖掘**它作为中介的传导作用（中介分析）。
A First Course in Causal Inference's Summary
--------------------

因果关系是非常值得考虑的。在自然科学的众多领域，无数自然界的规律都是通过实验来验证事物之间的因果关系。这包括物理学中的经典力学框架，化学中的元素周期律，生物中的演化论自然选择学说。我们从证据出发，来探究现实世界的因果关系。

在统计学中，我们的一切推断都是从数据出发。数据来源于现实的总体，包含着我们感兴趣的信息。通常来说，作为统计学人士的我们，面对数据中可能反映的因果关系有两种主要的思考：$Causal\ Discovery$和$Causal\ Inference$。

两者有什么区别呢？

$Discovery$在做的是我们通常所说的数据挖掘的部分，一切还是未知的状态，我们像考古学家一样，从获得的数据中试图拼凑出事物的原貌，或者找到更深层次的规律。

$Inference$就不一样了。在本书中，我们已经知道关心的$Treatment$处理变量会对$Outcome$结果变量产生影响，这是因为$Outcome$发生在$Treatment$之后。我们应该如何度量这种$Treatment\rightarrow Outcome$影响的大小，方向乃至可能存在的中间环节。这都是推断所关心的。

因果推断是很新的一门学科。统计学家Neyman在1923年自己的硕士论文中，首次提出完全随机化实验下的潜在结果概念。统计学家Rubin在1980年整理并发展，成为今天所熟知的鲁宾因果框架。2000年后，因果推断在社会科学的众多领域发挥出重要的作用，而国内却对因果推断尚无足够的认识。

本书是丁鹏教授的A First Course in Causal Inference的个人总结。没有过多的内容解释，只是对书中的整体框架的简单整理。

------------------------

完全随机化实验
------------------------
从实验设计的角度，完全随机化实验是探究因果作用能做的最好的方法。这建立在有限总体的框架下，随机性完全来自于实验分配的$Treatment$变量$Z$不同，潜在结果$Y(1),Y(0)$没有随机性。
$$Z\perp\!\!\!\perp\{Y(1),Y(0)\}$$
完全化随机实验所满足的条件：处理与潜在变量独立对正确进行因果推断有极大帮助

* Fisher考虑了如何检验$H_{0F}:Y_i(1)=Y_i(0)\ \forall i$，他给出了利用任意检验统计量得到$p_{FRT}=\dfrac{1}{M}\sum_{m=1}^MI\{T(z^m,Y)\geq T(Z,Y)\}$的方法。一个典型的统计量是两个有限总体的均值差异$\hat{\tau}=\hat{\overline{Y}}(1)-\hat{\overline{Y}}(0)$这说明此时完全转换为两样本均值之差的检验
  ```
  t.test(Y[Z==1],Y[Z==0],var.equal = FALSE)
  #或者wilcox秩和检验
  ```
* Neyman给出了平均因果作用的概念，以及无偏的点估计。$E[\hat{\tau}]=\tau=\dfrac{1}{n}\sum_{i=1}^n\tau_i$，虽然该估计量的方差估计有无法计算的一部分，但我们能得到的是更差的方差估计，在最坏情况下都能支持的结论也极富有意义。$E[\hat{V}]-var(\hat{\tau})=\dfrac{S^(\tau)}{n}\geq 0$。我们通常可以利用一个简单的线性回归得到估计量点估计，$\hat{\beta}=\hat{\tau}(simple\ matrix\ calculate)$，然而估计量的方差估计最好使用更稳健的EHW方法。
  ```
  方法1：tauhat = mean(y[z==1])-mean(y[z==0])
        varhat = var(y[z==1])/n1+var(y[z==0])/n0
  方法2: olsfit = lm(y~z)
        tauhat = olsfit$coef
        varhat = hccm(olsfit,type = "hc2")
  ```
如果只有$Z$和$Y$当然在完全随机化假设下很好得到结果，如果还有合适的协变量$X$，这额外的信息有助于我们得到更准确的结果。表现在数学中，$Var_{SRE}(\hat{\tau})\leq Var_{CRE}(\hat{\tau})$，如果有合适的协变量$X$可以引入，分层随机化实验总是比随机化实验更好。

然而，如果我们的协变量$X$多维，连续，选择有效的分层规则是困难乃至无意义的。这时$Regression\ Adjustment$给了我们利用协变量的机会。我们有$Lin(2013)'s\  estimator$。通过估计潜在结果中无法用协变量解释部分的差异$\hat{\tau}(\beta_1,\beta_0)=\{\hat{\overline{Y}}(1)-\beta_1\hat{\overline{X}}(1)\}-\{\hat{\overline{Y}}(0)-\beta_0\hat{\overline{X}}(0)\}$，值得注意的是，这种方法要求事前对所有协变量标准化，以满足平均值为0的推导要求，无论变量是离散还是连续的。
```
obtain the hattau(b1,b0) from a single OLS fit
hattau(b1,b0) = coefficient of Z in the OLS of Y ~ Z + X + Z*X
where X = scale(X)
```

----------------------------------------

unconfounded观察性研究
---------------------------------------
在观察性研究中，我们无法从实验设计的角度来重现完全随机化实验带来的$Z\perp\!\!\!\perp\{Y(1),Y(0)\}$，我们只能从已有的数据结果来尝试因果推断。
我们绝望地提出下面的强忽略性假设:
$$\{Y(1),Y(0)\}\perp\!\!\!\perp Z|X$$
在足够的协变量支持下，处理与潜在结果是条件独立。这说明我们把所有可能参与，影响$Z$与$Y$的协变量$X$都用上。也说明了没有混杂因素干扰我们的观察性研究。

在强忽略性假设下，我们有这些方法

* $Outcome\ Regression：$对结果变量使用回归模型是很直觉的，回归模型是统计学家最熟悉的。如果认为线性模型是合适的，就有$E[Y|Z,X]=\beta_0+\beta_zZ+\beta_xX$，这种情况下想要得到$\tau(X)=E[Y|Z=1,X]-E[Y|Z=0,X]$无异是简单的。觉得线性回归不好，试试别的也可以。这样松散的要求导致结果对模型的选择十分敏感。
* $IPW:$ 逆概率加权方法建立在倾向性得分$e(X)=P(Z=1|X)$，在$Overlap:e(X)\in(0,1)$假定下，$\tau=E\{Y(1)-Y(0)\}=E\left\{\dfrac{ZY}{e(X)}-\dfrac{(1-Z)Y}{1-e(X)}\right\}$，通常我们可以利用$pr(Z=1|X)=logistic(Z~X)$来获得$e(X)$进而获得$IPW$方法的估计量。倾向性得分的$Overlap$条件很容易在边界附近得到敏感结果，截断的方法也会时常使用。
* $Doubly\ Robust\ Esitimator：$综合利用上面两个估计量，我们可以得到双稳健的估计。在强忽略性假设和$Overlap$假设下，只要回归模型或者逆概率加权模型中有一个是正确的，就可以得到平均因果作用的无偏估计。引入$\mu(X,\beta),e(X,\alpha)$参数模型来构建统计量



------------------------------

confounded观察性研究
---------------------------------
如果有无法观测的混杂性因素存存在$(unmeasured\ confounding)$，此时前面所有建立在强忽略性假设下的方法都无用了。因为我们并没有考虑到所有的，可能影响因果路径的协变量$X$

我们无法使用数据来验证是否满足强可忽略性假设，在强可忽略性假设不成立的前提下，**敏感性分析**给了我们确定因果关系的工具。

* $E-Value:$如果有未观测到的混杂因素$U$，我们在$Z\perp\!\!\!\perp Y|(X,U)$的假定下，可以给出$max(RR_{ZU},RR_{UY})\geq RR_{ZY}^{obs}+\sqrt{RR_{ZY}^{obs}(RR_{ZY}^{obs}-1)}$，后者被称作$E-Value$，与Fisher在完全随机化实验中提出的p值相比，$E-Value$给出了观察性研究中更好的因果作用测量。它刻画了如果存在未观测的混杂变量$U$，$U$与$Z$或$Y$两个因果作用更大值的下。
* $Outcome\ regression,IPW,doubly\ robust\ esitimator$(参数方法)

通常考虑更多的协变量$X$会让强可忽略性假设更可信，相较而言，$Overlap$假定在很多情境下是无法满足的。很有可能某些个体的倾向性得分$e(X)=1\ or\ 0$。在$Overlap$假定不成立的情况下，利用条件期望极限结论的断点回归方法是得到认可的。

* $RD:$在边界点附近回归，得到某点处的平均因果效应$\tau(x_0)=\underset{\epsilon\rightarrow 0^+}{lim}E[Y|Z=1,X=x_0+\epsilon]-\underset{\epsilon\rightarrow 0^+}{lim}E[Y|Z=0,X=x_0-\epsilon]$这种方法也可以视作对不连续回归的敏感性分析。

如何从实验设计的角度来解决那些我们不知道的混杂变量带来的影响，工具变量方法是一个好选择。
* $Encouragement\ Design：$激励实验角度来考虑，利用联合潜在结果$\{D_i(1),D_i(0)\}$来对人群分层$U=a,n,c,d$，我们可以因此给出$complier$人群的平均因果作用$\tau_c=\dfrac{\tau_Y}{\tau_D}=\dfrac{E(Y|Z=1)-E(Y|Z=0)}{E(D|Z=1)-E(D|Z=0)}$
* $Linear\ Instrumental\ Variable\ Model：$计量经济学框架中利用因果图来说明可能得混杂变量影响，用两步最小二乘方法$Two-stage\ least\ squares$

-----------------------------------------

主分层方法与中介分析
--------------------------------------
在许多的应用中，有些特殊的变量发生处理后（比如生存分析中的是否存活变量），此时这种别称作后处理变量$M$会与结果变量$Y$相关，研究这种变量在因果机制中的作用是重要的。

对于后处理变量，使用主分层方法$Principal\ method$，这种方法的思想很直接，只比较在相同的后处理变量下$M=m$中的平均因果作用。对于二元的情形，$\tau(1,0)=E\{Y(1)-Y(0)|M(1)=1,M(0)=0\}$，同理我们也可以利用这种潜在联合结果给出$\tau(1,1),\tau(0,1),\tau(0,0)$。此处的结果与激励实验中对人群的分层类似，在后处理变量分析中，分层的是受处理影响的变量$M$

另一种研究的背景是关心因果路径$Z\rightarrow Y$上可能存在的中介因素，我们通常使用中介分析$Mediation Analysis$。利用嵌套潜在结果$Y(z,M_z)$，在composition假设下$Y(z,M_z)=Y(z),z=0,1$,考虑直接因果作用$NDE=E\{Y(1,M_0)-Y(0,M_0)\}$与间接因果作用$NIE=E\{Y(1,M_1)-Y(1,M_0)\}$，反正在composition假设下有$\tau=NIE+NDE$即可。
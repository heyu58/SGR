Boosting
===================================

**集成方法**在机器学习领域是一种通用技术，通过组合多个预测器来生成一个更准确的预测器。本章研究的是一种重要的集成方法——Boosting，更准确的是Adaboost算法（虽然目前[2024]业界更常用XGBoost算法）。

引言
------------------------
对于一个非平凡的学习任务来说，直接设计一个能满足强PAC学习要求的准确算法是非常困难的。但我们可以很容易找到一种简单的预测器，效果稍好于随机猜测。下面给出这种**弱学习器**的形式定义。

**弱学习**：*如果存在一个算法$\mathcal{A}，\gamma>0$以及一个多项式函数$poly()$，使得对于任意$\delta>0$，对于所有在样本空间$\mathcal{X}$上的分布$\mathcal{D}$以及任意目标概念$c\in C$，对于满足$m\geq poly\left(\dfrac{1}{\delta},n,size(c)\right)$的任意样本规模$m$均有下式成立，那么概念类$\mathcal{C}$弱是PAC可学习的*

$$\underset{S\sim\mathcal{D}^m}{\mathbb{P}}\Big[R(h_S)\leq\frac{1}{2}-\gamma\Big]\geq1-\delta$$

*其中，$h_s$是算法$\mathcal{A}$在样本集$S$上训练得到的假设。如果这样的算法$\mathcal{A}$存在，该算法被称为概念类$C$的弱学习算法或弱学习器。弱学习算法返回的假设被称为基分类器*

boosting方法的核心思想是通过弱学习算法来构造$强学习器$，即该算法准确的PAC学习算法。为达到此目的，boosting利用了集成方法：通过组合弱学习器返回的不同基分类器构造一个准确度更高的预测器。

AdaBoost算法
------------------------
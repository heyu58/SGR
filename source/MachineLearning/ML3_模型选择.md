模型选择
===================================
学习算法设计中一个关键问题是假设集合$\mathcal{H}$的选择，这被称为模型选择问题。

当一个样本的标签可以有某个唯一的可测函数$f:\mathcal{X}\rightarrow\mathcal{Y}$（以概率1成立时），这种情况被称作**确定性情境**。此时，考虑一个输入空间服从的分布$\mathcal{D}$，依分布$\mathcal{D}$采样得到的训练样本$(x_1,x_2,...,x_n)$，并且通过$f:y_i=f(x_i)，i\in[1,n]$确定对应的标签。很多学习问题都可以在此确定性情形下完成形式化。根据定义可知，存在一个没有泛化误差的目标函数使得$R(h)=0$

我们推广到更加常见的**随机性情境**。此时机器学习要解决的问题是，找到一个有较小泛化误差的假设$h\in\mathcal{H}$

$$R(h)=\underset{(x,y)\sim\mathcal{D}}{\mathbb{P}}[h(x)\neq y]$$

此时学习算法输出的标签是输入的一个概率函数。例如想用一个人的身高和体重推断这个人的性别，标签往往不是唯一的，对于大多数输入，男生和女生都是可能的性别。对于每个固定的输入(年龄，身高，体重...)，其输出的标签为男性对应的是一个概率分布。

**贝叶斯误差:** *给定一个在$\mathcal{X\times Y}$上的分布$\mathcal{D}$，响应的贝叶斯误差$R^*$定义为由可测函数类$h:\mathcal{X\rightarrow Y}$产生的误差下界：*

$$R^*=\underset{h(可测)}{inf} R(h)$$

*一个满足$R(h)=R^*$的假设$h$被称为贝叶斯假设或贝叶斯分类器。*

根据上述定义，对于确定性情境$R^*=0$，对于随机性情境$R^*\neq 0$

------------------------------------
如何选择假设集$\mathcal{H}$?一个足够丰富或复杂的假设集可以包含理想的贝叶斯分类器。但是这样的族是充满困难的。更一般的说，假设集$\mathcal{H}$的选择需要权衡利弊，可以通过**估计**和**近似误差**进行分析。

我们的讨论将集中的二元分类的特殊情况喜爱，但可以直接扩展到不同的任务和损失函数。

估计误差和近似误差
-------------------------------
令$\mathcal{H}$是一个可以将$\mathcal{X}$映射到$\{0,1\}$的函数族。从$\mathcal{H}$中选取的假设的误差，即误差$R(h)$和贝叶斯误差$R^*$的差，可以按照下式进行分解：

$$R(h)-R^*=\underset{估计误差}{\left[R(h)-\underset{h\in\mathcal{H}}{infR(h)}\right]}+\underset{近似误差}{\left[\underset{h\in\mathcal{H}}{infR(h)}-R^*\right]}$$

估计误差依赖于所选定的假设$h$。他相对于由$\mathcal{H}$中的假设获得的误差下确界来测量$h$的误差，或达到下确界时磊中的最优家色号$h^*$的误差。**注意！之前的不可知PAC学习的定义便是基于估计误差的。**

近似误差衡量了用$\mathcal{H}$对贝叶斯误差的近似程度。这种误差可以看做假设集$\mathcal{H}$的一个性质，衡量假设集的丰富程度。对于更复杂或更丰富的假设$\mathcal{H}$，近似误差的减少往往以较大的估计误差作为代价。

![模型选择](image/估计误差与近似误差.png "图片title")

